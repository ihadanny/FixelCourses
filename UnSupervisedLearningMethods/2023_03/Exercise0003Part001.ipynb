{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/qkg2E2D.png)\n",
    "\n",
    "# UnSupervised Learning Methods\n",
    "\n",
    "## Exercise 003 - Part I\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 14/05/2023 | Royi Avital | First version                                                      |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/Exercise0002Part001.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    " - Fill the full names and ID's of the team members in the `Team Members` section.\n",
    " - Answer all questions / tasks within the Jupyter Notebook.\n",
    " - Use MarkDown + MathJaX + Code to answer.\n",
    " - Verify the rendering on VS Code.\n",
    " - Submission in groups (Single submission per group).\n",
    " - You may and _should_ use the forums for questions.\n",
    " - Good Luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "\n",
    " - `TAL_BARNAHOR_311151468`.\n",
    " - `OMRI_HOMBURGER_204085815`.\n",
    " - `IDO_HADANNY_034537969`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Principle Component Analysis (PCA)\n",
    "\n",
    "Let $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ be a diagonalizable matrix, that is $\\boldsymbol{A} = \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{-1}$ where $\\boldsymbol{\\Lambda}$ is a diagonal matrix.\n",
    "\n",
    "### 1.1. Question\n",
    "\n",
    "Prove the following:\n",
    "\n",
    "$$ \\operatorname{Tr} \\left( A \\right) = \\sum_{i = 1}^{d} {\\lambda}_{i} \\left( A \\right) $$\n",
    "\n",
    "Where $\\operatorname{Tr} \\left( A \\right) = \\sum_{i = 1}^{d} {A}_{ii}$ and ${\\lambda}_{i} \\left( \\boldsymbol{A} \\right) = {\\Lambda}_{ii}$ is the $i$ -th eigen value of $\\boldsymbol{A}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\operatorname{Tr}(A) = \\operatorname{Tr}(Q \\Lambda Q^{-1})$\n",
    "\n",
    "$ = \\operatorname{Tr}(Q^{-1} Q \\Lambda)$ (cyclic property of the trace)\n",
    "\n",
    "$ = \\operatorname{Tr}(I \\Lambda)$\n",
    "\n",
    "$ = \\operatorname{Tr}(\\Lambda)$\n",
    "\n",
    "$\\operatorname{Tr}(A) = \\sum_{i=1}^{d} \\Lambda_{ii}$ (trace of a diagonal matrix)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two square matrices $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ and $\\boldsymbol{B} \\in \\mathbb{R}^{d \\times d}$ are called similar, namely $ \\boldsymbol{A} \\sim \\boldsymbol{B}$ if there exist an invertible matrix $\\boldsymbol{P} \\in \\mathbb{R}^{d \\times d}$ such that:\n",
    "\n",
    "$$ \\boldsymbol{B} = \\boldsymbol{P} \\boldsymbol{A} \\boldsymbol{P}^{-1} $$\n",
    "\n",
    "* <font color='brown'>(**#**)</font> It means all matrices which are diagonalizable are similar to their respective diagonal matrix.\n",
    "\n",
    "### 1.2. Question\n",
    "\n",
    "Prove that if $\\boldsymbol{A}$ is diagonalizable and $\\boldsymbol{A} \\sim \\boldsymbol{B}$ then $\\boldsymbol{A}$ and $\\boldsymbol{B}$ share the same eigen values, namely:\n",
    "\n",
    "$$ \\boldsymbol{A} \\sim \\boldsymbol{B} \\implies {\\left\\{ {\\lambda}_{i} \\left( \\boldsymbol{A} \\right) \\right\\}}_{i = 1}^{d} = {\\left\\{ {\\lambda}_{i} \\left( \\boldsymbol{B} \\right) \\right\\}}_{i = 1}^{d} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Solution\n",
    "\n",
    "for any eigen-value $\\lambda_i$ of A, we have \n",
    "\n",
    "$Au_i = \\lambda_i u_i$ for some eigen-vector $u_i$. \n",
    "\n",
    "Now we can write:\n",
    "\n",
    "$PBP^{-1}u_i = \\lambda_i u_i$\n",
    "\n",
    "$BP^{-1}u_i = \\lambda_i P^{-1}u_i$\n",
    "\n",
    "so for the vector $v_i = P^{-1}u_i$ we have:\n",
    "\n",
    "$Bv_i = \\lambda_i v_i$\n",
    "\n",
    "which means it is an eigen-vector of $B$ with eigen-value $\\lambda_i$.\n",
    "\n",
    "We did not have to use the fact that $A$ is diagonaizable, except perhaps for assuming that it has $d$ eigen-values.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A symmetric matrix $\\boldsymbol{A} = \\boldsymbol{A}^{T}$ is called a _Symmetric Positive Definite_ (SPD) matrix if either:\n",
    "\n",
    "* All eigen values are positive: $\\forall i: \\; {\\lambda}_{i} \\left( \\boldsymbol{A} \\right) > 0$.\n",
    "* Any quadratic form is positive: $\\forall \\boldsymbol{v} \\neq \\boldsymbol{0}: \\; \\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} > 0$.\n",
    "\n",
    "Namely $\\boldsymbol{A} \\succ 0$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> There is also a _Symmetric Semi Positive Definite Matrix_ (SPSD) which obeys the above with weak inequality.\n",
    "\n",
    "### 1.3. Question\n",
    "\n",
    " 1. Prove the equivalency of the 2 properties, namely:\n",
    "\n",
    "$$ \\forall i: \\; {\\lambda}_{i} \\left( \\boldsymbol{A} \\right) > 0 \\iff  \\forall \\boldsymbol{v} \\neq \\boldsymbol{0}: \\; \\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} > 0$$\n",
    "\n",
    " 2. Prove or disprove: $\\boldsymbol{A}^{-1}$ is a _Symmetric Positive Definite_ matrix.\n",
    " 3. Prove or disprove: The SVD of $\\boldsymbol{A} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^{T}$ is also an eigen decomposition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Solution\n",
    "\n",
    "1. \n",
    "\n",
    "$ \\forall i: \\; {\\lambda}_{i} \\left( \\boldsymbol{A} \\right) > 0 \\Rightarrow  \\forall \\boldsymbol{v} \\neq \\boldsymbol{0}: \\; \\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} > 0$\n",
    "\n",
    "for any non-zero vector $v$, we can write v as a linear combination of the eigen-vectors of $A$:\n",
    "\n",
    "$v = \\sum_i c_iu_i$\n",
    "\n",
    "$v^TAv = (\\sum_i c_iu_i)^T A (\\sum_j c_ju_j)$\n",
    "\n",
    "$ = \\sum_i\\sum_j c_ic_j u_i^T A c_j$\n",
    "\n",
    "$ = \\sum_i\\sum_j c_ic_j\\lambda_j u_i^Tu_j$ (as they are eigen-vectors)\n",
    "\n",
    "$ = \\sum_i c_i^2\\lambda_i u_i^Tu_i$ (any eigen-vectors with different eigen-values are orthogonal)\n",
    "\n",
    "$ = \\sum_i c_i^2\\lambda_i$ (eigen-vectors norm is unitary)\n",
    "\n",
    "which is a sum of positive terms since $c_i^2$ is non-negative and $\\lambda_i$ are all positive by the assumption.\n",
    "\n",
    "\n",
    "$ \\forall i: \\; {\\lambda}_{i} \\left( \\boldsymbol{A} \\right) > 0 \\Leftarrow  \\forall \\boldsymbol{v} \\neq \\boldsymbol{0}: \\; \\boldsymbol{v}^{T} \\boldsymbol{A} \\boldsymbol{v} > 0$\n",
    "\n",
    "let $v$ be an eigen-vector, we know that:\n",
    "\n",
    "$ v^TAv > 0 $\n",
    "\n",
    "$ \\lambda v^Tv > 0 $ (where $\\lambda$ is the corresponding eigen-value)\n",
    "\n",
    "$ \\lambda ||v|| > 0 $ \n",
    "\n",
    "but we know that the norm $||v||$ must be non-negative, so $\\lambda$ must be positive.\n",
    "\n",
    "2. \n",
    "\n",
    "proof: let $A$ be an SPD matrix, and let $v$ be an eigen-vector of it, with eigen-value $\\lambda$. We know that:\n",
    "\n",
    "$Av = \\lambda v $\n",
    "\n",
    "$A^{-1}Av = \\lambda A^{-1}v $ (multiply both sides by $A^{-1}$)\n",
    "\n",
    "$v = \\lambda A^{-1}v $ \n",
    "\n",
    "$A^{-1}v = \\frac{1}{\\lambda} v $ \n",
    "\n",
    "so $v$ is also an eigen-vector of $A^{-1}$ with eigen-value $\\frac{1}{\\lambda}$, which is also positive. we get $d$ positive eigen-values of $A^{-1}$, so it must be that its also SPD, according to section 1.\n",
    "\n",
    "3. \n",
    "\n",
    "proof: we know that for the SVD decomposition of **any** matrix $U$ and $V^T$ are the eigen-vectors of $AA^T$ and $A^TA$ respectively, and the singular values in $\\Sigma$ are the square roots of the eigen-values of $AA^T$. \n",
    "\n",
    "Since $A$ is symmetric we know that it is diagonaziable and there is an eigen-decomposition where $A = Q\\Lambda Q^T$ where Q is orthogonal. \n",
    "\n",
    "so $AA^T = Q\\Lambda Q^TQ\\Lambda Q^T = Q\\Lambda^2Q^T$, and we can see that the columns of Q are the eigen-vectors of both $A$ and $AA^T$. We can also see that the eigen-values are the squares of the eigen-values of $A$. \n",
    "\n",
    "Now, since $A$ is not only symmetric but also definite positive, we know that its eigen-values are real positives, so the singular values which are square-roots of $\\Lambda^2$ are actually the eigen-values of $A$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Consider the data $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{R}^{D} \\right\\}_{i = 1}^{N}$ with mean $\\boldsymbol{\\mu}_{x} \\in \\mathbb{R}^{D}$ and covariance $\\boldsymbol{\\Sigma}_{x} \\in \\mathbb{R}^{D \\times D}$.\n",
    " * Let $\\boldsymbol{\\Sigma}_{x} = \\boldsymbol{U} \\boldsymbol{\\Lambda} \\boldsymbol{U}^{T}$ be the eigen decomposition of $\\boldsymbol{\\Sigma}_{x}$.\n",
    " * Let $\\boldsymbol{z}_{i} = \\boldsymbol{U}^{T} \\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)$.\n",
    "\n",
    "\n",
    "### 1.4. Question\n",
    "\n",
    "Prove the following:\n",
    "\n",
    " 1. The mean of the set $\\mathcal{Z} = \\left\\{ \\boldsymbol{z}_{i} \\in \\mathbb{R}^{D} \\right\\}$ is zero, that is, $\\boldsymbol{\\mu}_{z} = \\frac{1}{N} \\sum_{i = 1}^{N} \\boldsymbol{z}_{i} = 0$.\n",
    " 2. The covariance of $\\mathcal{Z}$ is diagonal, that is, $\\boldsymbol{\\Sigma}_{z}$ is diagonal.\n",
    " 3. The pair wise distance is equal, that is, $\\forall i, j: \\; {\\left\\| \\boldsymbol{z}_{i} - \\boldsymbol{z}_{j} \\right\\|}_{2} = {\\left\\| \\boldsymbol{x}_{i} - \\boldsymbol{x}_{j} \\right\\|}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Solution\n",
    "\n",
    "1.\n",
    "\n",
    "$\\mu_z = \\frac{1}{N}\\sum z_i = \\frac{1}{N}\\sum U^T(x_i - \\mu_x) = U^T [ \\frac{1}{N}\\sum(x_i) - \\mu_x ] = U^T [ \\mu_x - \\mu_x ] = 0$\n",
    "\n",
    "2.\n",
    "\n",
    "$ \\Sigma_z = \\frac{1}{N}\\sum z_i z_i^T = \\frac{1}{N} \\sum U^T(x_i-\\mu_x)(x_i-\\mu_x)U = $\n",
    "\n",
    "$ = \\frac{1}{N} U^T \\sum (x_i-\\mu_x)(x_i-\\mu_x) U $ rearranging the sum\n",
    "\n",
    "$ = U^T \\Sigma_x U $ by definition\n",
    "\n",
    "$ = U^T U \\Lambda U^T U $ eigen-decomposition\n",
    "\n",
    "$ = \\Lambda $ because U is orthogonal\n",
    "\n",
    "3. \n",
    "\n",
    "$ |z_i - z_j|_2 = | U^T(x_i - \\mu_x) - U^T(x_j - \\mu_x) |_2 $\n",
    "\n",
    "$ = | U^T(x_i - \\mu_x - x_j + \\mu_x) | _2 $\n",
    "\n",
    "$ = | U^T(x_i - x_j) |_2 $\n",
    "\n",
    "$ = (x_i - x_j)^T UU^T(x_i-x_j) $\n",
    "\n",
    "$ = (x_i - x_j)^T (x_i-x_j) $\n",
    "\n",
    "$ = |x_i - x_j|_2 $\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Let $\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}$ be a full rank matrix with $d \\leq D$. \n",
    "\n",
    " * <font color='brown'>(**#**)</font> If a matrix has rank $d$ it means its SVD  has $d$ non zero singular values.\n",
    "\n",
    "\n",
    "### 1.5. Question\n",
    "\n",
    "Show that exists an invertible matrix $\\boldsymbol{M} \\in \\mathbb{R}^{d \\times d}$ such that $\\boldsymbol{O} = \\boldsymbol{U}_{d} \\boldsymbol{M} \\in \\mathbb{R}^{D \\times d}$ is semi orthogonal, that is $\\boldsymbol{O}^{T} \\boldsymbol{O} = \\boldsymbol{I}_{d}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Consider the data $\\boldsymbol{X} \\in \\mathbb{R}^{D \\times N}$.\n",
    " * Assume it has zero mean $\\boldsymbol{\\mu}_{x} = \\boldsymbol{X} \\boldsymbol{1}_{N} = \\boldsymbol{0}$.\n",
    " * Its covariance is given by $\\boldsymbol{\\Sigma}_{x} = \\frac{1}{N} \\boldsymbol{X} \\boldsymbol{X}^{T}$.\n",
    "\n",
    "\n",
    "### 1.6. Question (Bonus 4 Points)\n",
    "\n",
    "Prove the following 2 optimization problem are equivalent.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\arg \\min_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & {\\left\\| \\boldsymbol{X} - \\boldsymbol{U}_{d} \\boldsymbol{U}_{d}^{T} \\boldsymbol{X} \\right\\|}_{F}^{2} \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{U}^{T} \\boldsymbol{U} & = \\boldsymbol{I}_{d} \\\\\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\arg \\max_{\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}} \\quad & \\operatorname{Tr} \\left( \\boldsymbol{U}_{d}^{T} \\boldsymbol{\\Sigma}_{x} \\boldsymbol{U}_{d} \\right) \\\\\n",
    "\\text{subject to} \\quad & \\begin{aligned} \n",
    "\\boldsymbol{U}^{T} \\boldsymbol{U} & = \\boldsymbol{I}_{d} \\\\\n",
    "\\end{aligned}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    " * <font color='brown'>(**#**)</font> Equivalent means the have the same solution, in the case above, same minimizer.\n",
    " * <font color='brown'>(**#**)</font> The above shows the equivalence between the trace form, which decorrelates the data, and the encoder decoder form, which minimized the reconstruction error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Consider the data $\\mathcal{X} = \\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$ with mean $\\boldsymbol{\\mu}_{x} \\in \\mathbb{R}^{D}$ and covariance $\\boldsymbol{\\Sigma}_{x} \\in \\mathbb{R}^{D \\times D}$.\n",
    " * Let $\\boldsymbol{U}_{d} \\in \\mathbb{R}^{D \\times d}$ be a semi orthogonal matrix, that is, $\\boldsymbol{U}_{d}^{T} \\boldsymbol{U}_{d} = \\boldsymbol{I}$.\n",
    " * Where $\\boldsymbol{U}_{d}$ be the $d$ eigen vectors corressponding to the $d$ largest eigen values of $\\boldsymbol{\\Sigma}_{x}$.\n",
    " * Let $\\boldsymbol{z}_{i} = \\boldsymbol{U}_{d}^{T} \\left( \\boldsymbol{x}_{i} - \\boldsymbol{\\mu}_{x} \\right)$.\n",
    " * Let $\\hat{\\boldsymbol{x}}_{i} = \\boldsymbol{U}_{d} \\boldsymbol{z}_{i} + \\boldsymbol{\\mu}_{x}$.\n",
    " * Let $\\boldsymbol{\\epsilon}_{i} = \\hat{\\boldsymbol{x}}_{i} - \\boldsymbol{x}_{i}$.\n",
    "\n",
    "\n",
    "### 1.7. Question\n",
    "\n",
    "Prove the following:\n",
    "\n",
    " 1. $\\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{x} \\right) = \\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{z} \\right) + \\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{\\epsilon} \\right)$.\n",
    " 2. $\\operatorname{Tr} \\left( \\boldsymbol{\\Sigma}_{\\epsilon} \\right) = \\sum_{i = d + 1}^{D} \\lambda_{i} \\left( \\boldsymbol{\\Sigma}_{x} \\right)$.\n",
    "\n",
    "Where\n",
    "\n",
    " * $\\boldsymbol{\\Sigma}_{z} \\in \\mathbb{R}^{d \\times d}$ is the covariance of $\\left\\{ \\boldsymbol{z}_{i} \\right\\}_{i = 1}^{N}$.\n",
    " * $\\boldsymbol{\\Sigma}_{\\epsilon} \\in \\mathbb{R}^{D \\times D}$ is the covariance of $\\left\\{ \\boldsymbol{\\epsilon}_{i} \\right\\}_{i = 1}^{N}$.\n",
    "\n",
    "</br>\n",
    "\n",
    " * <font color='brown'>(**#**)</font> The idea is to prove the total variance / energy is kept. Part of it in the low dimensionality data and the other is in the error (Lost information).\n",
    " * <font color='brown'>(**#**)</font> To make calculations easier, think how $\\boldsymbol{\\mu}_{x}$ effects the variance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Let $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ be a square matrix.\n",
    " * Let $\\boldsymbol{u}_{1}, \\boldsymbol{u}_{2} \\in \\mathbb{R}^{d}$ be eigen vectors with the same eigen value $\\lambda$ such that $\\boldsymbol{A} \\boldsymbol{u}_{1} = \\lambda \\boldsymbol{u}_{1}$ and $\\boldsymbol{A} \\boldsymbol{u}_{2} = \\lambda \\boldsymbol{u}_{2}$.\n",
    " * The vectors $\\boldsymbol{u}_{1}, \\boldsymbol{u}_{2}$ are perpendicular, that is, $\\left \\langle \\boldsymbol{u}_{1}, \\boldsymbol{u}_{2} \\right \\rangle = 0$.\n",
    " * Let $\\boldsymbol{v} = \\alpha \\boldsymbol{u}_{1}$ where $\\alpha \\neq 0$.\n",
    "\n",
    "\n",
    "### 1.10. Question\n",
    "\n",
    " 1. Show $\\boldsymbol{v}$ is an eigen vector of $\\boldsymbol{A}$ and find its corresponding eigen value.\n",
    " 2. Prove or disprove that $\\boldsymbol{u} = \\boldsymbol{u}_{1} + \\boldsymbol{u}_{2}$ is an eigen vector of $A$.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Let $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$ be a diagonalizable matrix.\n",
    " * Let $\\left\\{ \\left( \\boldsymbol{u}_{i}, {\\lambda}_{i} \\right) \\right\\}_{i = 1}^{d}$ be the set of eigen pairs, that is, $\\boldsymbol{A} \\boldsymbol{u}_{i} = {\\lambda}_{i} \\boldsymbol{u}_{i}$.\n",
    " * Let $\\boldsymbol{B} = \\boldsymbol{R} \\boldsymbol{A} \\boldsymbol{R}^{T}$ where $\\boldsymbol{R} \\in \\mathbb{R}^{d \\times d}$ is an orthogonal matrix, namely, $\\boldsymbol{R}^{T} \\boldsymbol{R} = \\boldsymbol{R} \\boldsymbol{R}^{T} = \\boldsymbol{I}$.\n",
    "\n",
    "\n",
    "### 1.11. Question\n",
    "\n",
    "Find the set $\\left\\{ \\left( \\boldsymbol{v}_{i}, {\\alpha}_{i} \\right) \\right\\}_{i = 1}^{d}$ of eigen pairs of $\\boldsymbol{B}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Consider the data $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{10} \\right\\}_{i = 1}^{N}$ and its matrix form $\\boldsymbol{X} \\in \\mathbb{R}^{10 \\times N}$.\n",
    " * Let $\\boldsymbol{\\Sigma}_{x} \\in \\mathbb{R}^{10 \\times 10}$ be the covariance matrix where all of its eigen values are unique.\n",
    " * Let $\\left\\{ \\boldsymbol{z}_{i} \\in \\mathbb{R}^{3} \\right\\}_{i = 1}^{N}$ be the low dimensional representation obtained by applying PCA from $\\mathbb{R}^{10}$ to $\\mathbb{R}^{3}$\n",
    " * Let $\\left\\{ \\boldsymbol{w}_{i} \\in \\mathbb{R}^{2} \\right\\}_{i = 1}^{N}$ be the low dimensional representation obtained by applying PCA from $\\mathbb{R}^{10}$ to $\\mathbb{R}^{2}$\n",
    "\n",
    "\n",
    "### 1.12. Question\n",
    "\n",
    "Prove or disprove:\n",
    "\n",
    "$$ \\boldsymbol{w}_{i} = \\begin{bmatrix}\n",
    "{\\alpha}_{1} & 0 & 0 \\\\ \n",
    "0 & {\\alpha}_{2} & 0\n",
    "\\end{bmatrix} \\boldsymbol{z}_{i}, \\; {\\alpha}_{1}, {\\alpha}_{2} \\in \\left\\{ -1, 1 \\right\\} $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kernel Principle Component Analysis (K-PCA)\n",
    "\n",
    "Let $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T} \\in \\mathbb{R}^{N \\times N}$ be the centering matrix.\n",
    "\n",
    "### 2.1. Question\n",
    "\n",
    "Prove that $\\boldsymbol{J}$ is an idempotent matrix, that is, $\\boldsymbol{J}^{k} = \\boldsymbol{J}$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> This implies that if we center the data multiple times it will have no effect beyond doing it once."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Let $\\boldsymbol{X} \\in \\mathbb{R}^{D \\times N}$.\n",
    " * The covariance matrix $\\boldsymbol{\\Sigma}_{x} = \\boldsymbol{X} \\boldsymbol{X}^{T} \\in \\mathbb{R}^{D \\times D}$.\n",
    " * The matrix $\\boldsymbol{K}_{x} = \\boldsymbol{X}^{T} \\boldsymbol{X} \\in \\mathbb{R}^{N \\times N}$.\n",
    " * Let $\\left( \\boldsymbol{u}_{i}, {\\lambda}_{i} > 0 \\right)$ be an eigen pair of $\\boldsymbol{\\Sigma}_{x}$, such that, $\\boldsymbol{\\Sigma}_{x} \\boldsymbol{u}_{i} = {\\lambda}_{i} \\boldsymbol{u}_{i}$.\n",
    "\n",
    "\n",
    "### 2.2. Question\n",
    "\n",
    " 1. Show that ${\\lambda}_{i}$ is an eigen value of $\\boldsymbol{K}_{x}$.\n",
    " 2. Find the corresponding eigen vector $\\boldsymbol{v}_{i}$ such that $\\boldsymbol{K}_{x} \\boldsymbol{v}_{i} = {\\lambda}_{i} \\boldsymbol{v}_{i}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Kernel Functions**\n",
    " \n",
    " * Let $k: \\mathbb{R}^{d} \\times \\mathbb{R}^{d} \\to \\mathbb{R}$.\n",
    " * Consider $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{N}$.\n",
    "\n",
    "### 2.3. Question\n",
    "\n",
    "Show that if $k \\left( \\cdot, \\cdot \\right)$ can be written as inner product:\n",
    "\n",
    "$$ k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) = \\left \\langle \\phi \\left( \\boldsymbol{x}_{i} \\right), \\phi \\left( \\boldsymbol{x}_{j} \\right) \\right \\rangle $$\n",
    "\n",
    "for some $\\phi : \\mathbb{R}^{d} \\to \\mathbb{R}^{M} $ then the matrix defined by $\\boldsymbol{K} \\left[ i, j \\right] = k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) $ is _Symmetric Positive Semi Definite_ (SPSD), that is, $\\boldsymbol{K} \\succeq 0$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Let $\\boldsymbol{A} \\succ 0 $ be an SPD matrix.\n",
    " * Let $k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right) = \\boldsymbol{x}_{i}^{T} \\boldsymbol{A} \\boldsymbol{x}_{j} $.\n",
    "\n",
    "### 2.4. Question\n",
    "\n",
    "Prove or disprove that $k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$ is a kernel function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let $k \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right) = {\\left( 1 + \\boldsymbol{x}^{T} \\boldsymbol{y} \\right)}^{2}$.\n",
    "\n",
    "### 2.5. Question\n",
    "\n",
    "Prove or disprove that $k \\left( \\boldsymbol{x}, \\boldsymbol{y} \\right)$ is a kernel function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " * Let $\\boldsymbol{K}_{x} \\left[ i, j \\right] = k \\left( \\boldsymbol{x}_{i}, \\boldsymbol{x}_{j} \\right)$ be an **SPD** kernel matrix.\n",
    " * Let $\\tilde{\\boldsymbol{K}}_{x} = \\boldsymbol{J} \\boldsymbol{K}_{x} \\boldsymbol{J}$ be a centered kernel matrix where $\\boldsymbol{J} = \\boldsymbol{I} - \\frac{1}{N} \\boldsymbol{1} \\boldsymbol{1}^{T}$.\n",
    "\n",
    "### 2.7. Question\n",
    "\n",
    "Prove or disprove: $\\tilde{\\boldsymbol{K}}_{x}$ is an SPD matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Out of Sample Extension**\n",
    "\n",
    " * Consider the training set $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathbb{R}^{D} \\right\\}_{i = 1}^{N}$.\n",
    " * Let $\\boldsymbol{K}_{x}$ be the kernel matrix obtained by applying the kernel on the training set.\n",
    " * Let $\\boldsymbol{J} \\boldsymbol{K} \\boldsymbol{K} = \\boldsymbol{V} \\boldsymbol{\\Sigma}^{2} \\boldsymbol{V}^{T}$ be the eigen decomposition of the centered kernel matrix.\n",
    " * Let $\\boldsymbol{Z} \\in \\mathbb{R}^{d \\times N}$ be the low dimensional representation obtained by applying K-PCA, that is, $\\boldsymbol{Z} = \\boldsymbol{\\Sigma}_{d} \\boldsymbol{V}_{d}^{T}$.\n",
    "\n",
    "</br>\n",
    "\n",
    " * <font color='brown'>(**#**)</font> Kernel matrix is also called _Gram Matrix_.\n",
    "\n",
    "### 2.8. Question\n",
    "\n",
    "Let $\\boldsymbol{X}^{\\star} \\in \\mathbb{R}^{D \\times M}$ be a set of a new unseen data points. Write an expression, in a matrix form, for $\\boldsymbol{Z}^{\\star} \\in \\mathbb{R}^{d \\times M}$, the K-PCA out of sample extension applied to $\\boldsymbol{X}^{\\star}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
